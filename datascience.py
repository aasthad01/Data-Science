# -*- coding: utf-8 -*-
"""DataScience

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zvPb2MaKNSe0cs3w74QL8F64U5g6hrgo
"""

!pip install mlxtend
!pip install -U imbalanced-learn

# Importing Packages

import pandas as pd
import numpy as np
import random
import matplotlib.pyplot as plt
from imblearn.over_sampling import RandomOverSampler
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import plot_tree
from sklearn.preprocessing import StandardScaler
from tensorflow import keras as tfk
from sklearn.cluster import KMeans
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import statsmodels.api as sm
from statsmodels.stats.outliers_influence import variance_inflation_factor
from sklearn.decomposition import PCA
from sklearn.linear_model import LogisticRegression
from mlxtend.frequent_patterns import apriori
from mlxtend.frequent_patterns import association_rules
from mlxtend.preprocessing import TransactionEncoder
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import confusion_matrix, classification_report

from google.colab import files
uploaded = files.upload()

# Getting Data

dataset = pd.read_csv("car-sales-extended-missing-data.csv")

# Executing Commands

print(dataset.head)

# Accessing Records and Variables

record=dataset.Price
print(record)
row_index = 5
row = dataset.iloc[row_index]
print(row)

# Add an Index Field

dataset.set_index("Make", inplace = True)
print(dataset)

# Change Misleading Field Values

#df = dataset.dropna()
dataset['Colour'] = dataset['Colour'].replace('red', 'black')
dataset = dataset.fillna(dataset.apply(lambda x: np.random.choice(x.dropna()), axis=0))
print(dataset)

# Reexpress Categorical Field Values

category_mapping = {'White' : 'WTE', 'Blue' : 'BLU', 'Black' : 'BLCK'}
dataset['Colour'] = dataset['Colour'].map(category_mapping)
print(dataset)

# Standardise Numeric Fields

d = pd.DataFrame(dataset)
d['Price'] = (d['Price'] - d['Price'].mean()) / d['Price'].std()
print(d['Price'])
print(dataset)

# Identify Outliers

Q1 = dataset['Price'].quantile(0.25)
Q3 = dataset['Price'].quantile(0.75)
IQR = Q3 - Q1
lower_threshold = Q1 - 1.5 * IQR
upper_threshold = Q3 + 1.5 * IQR
outliers = dataset[(dataset['Price'] < lower_threshold) | (dataset['Price'] > upper_threshold)]
print(outliers)

# Construct a Bar Graph with Overlay

#Sample DataFrame (replace this with your actual DataFrame)
data = {
    'Make': ['Honda', 'BMW', 'Toyota', 'Nissan', 'Toyota', 'Honda', 'Toyota', 'Nissan', 'Honda', 'Toyota'],
    'Color': ['White', 'Blue', 'White', 'White', 'Blue', 'Black', 'White', 'Blue', 'White', 'Blue'],
    'Odometer': [76343.0, 192714.0, 84714.0, 154365.0, 181577.0, 35820.0, 155144.0, 66604.0, 215883.0, 248360.0],
    'Doors': [4.0, 5.0, 4.0, 4.0, 4.0, 4.0, 3.0, 4.0, 4.0, 4.0],
    'Random': [-0.121441, 0.424357, 0.685678, 0.685678, -0.272657, 1.853708, -1.256392, 1.797946, -1.458998, -0.427536]
}

df = pd.DataFrame(data)
column_to_plot = 'Odometer'

#Create a bar graph with overlay
plt.figure(figsize=(10, 6))
sns.barplot(data=df, x='Make', y=column_to_plot, ci=None, palette='Set1')

#Customize the plot
plt.title(f'Bar Graph of {column_to_plot} by Make')
plt.xlabel('Make')
plt.ylabel(column_to_plot)

#Show the plot
plt.show()

# Construct Contingency Tables

data_crosstab = pd.crosstab(dataset['Colour'], dataset['Doors'], margins = False)
print(data_crosstab)

# Construct Histograms with Overlay

column_to_plot = 'Odometer'

#Convert column to numerical data type
df[column_to_plot] = pd.to_numeric(df[column_to_plot], errors='coerce')

#Create a histogram
plt.figure(figsize=(10, 6))
sns.histplot(data=df, x=column_to_plot, hue='Make', multiple='stack', bins=30, kde=True)

#Customize the plot
plt.title(f'Overlay Histogram of {column_to_plot} by Make')
plt.xlabel(column_to_plot)
plt.ylabel('Frequency')
plt.legend(title='Make')

#Show the plot
plt.show()

# Perform Binning Based on Predictive Value

data['new_bin'] = pd.qcut(data['Odometer'], q=3)
print(data)

# Perform Model Evaluation

df = dataset.dropna()

#Define features and target variable
X = df[['Odometer (KM)', 'Doors']]
y = df['Price']

#Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

#Initialize and train the linear regression model
model = LinearRegression()
model.fit(X_train, y_train)

#Make predictions on the test set
y_pred = model.predict(X_test)

#Evaluate the performance of the regression model
mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

#Display the evaluation metrics
print(f"Mean Absolute Error: {mae:.2f}")
print(f"Mean Squared Error: {mse:.2f}")
print(f"R-squared: {r2:.2f}")

# Accounting for Unequal Error Costs

df = dataset.dropna()

#Define features and target variable
X = df[['Odometer (KM)', 'Doors', 'Price']]
y = df['Doors']

#Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

#Initialize and train the logistic regression model
model = LogisticRegression(random_state=42)
model.fit(X_train, y_train)

#Make predictions on the test set
y_pred = model.predict(X_test)

#Evaluate the performance of the classification model with default threshold
print("Default Threshold:")
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))

#Define unequal error costs
cost_false_positive = 1  # Cost of a false positive
cost_false_negative = 5  # Cost of a false negative

#Adjust the decision threshold based on error costs
decision_threshold = cost_false_negative / (cost_false_positive + cost_false_negative)
y_pred_adjusted = (model.predict_proba(X_test)[:, 1] > decision_threshold).astype(int)

#Evaluate the performance of the classification model with adjusted threshold
print("\nAdjusted Threshold:")
print(confusion_matrix(y_test, y_pred_adjusted))
print(classification_report(y_test, y_pred_adjusted))

# Application of Naïve Bayes

df = dataset.dropna()

#Define features and target variable
X = df[['Odometer (KM)', 'Doors', 'Price']]
y = df['Colour']

#Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

#Initialize the Naïve Bayes classifier (assuming Gaussian Naïve Bayes in this case)
nb_classifier = GaussianNB()

#Train the classifier
nb_classifier.fit(X_train, y_train)

#Make predictions on the test set
y_pred = nb_classifier.predict(X_test)

#Evaluate the performance of the classifier
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.2f}")

#Display classification report
print(classification_report(y_test, y_pred))

# Application of NEURAL NETWORKS

df = dataset.dropna()

#Define features and target variable
X = df[['Odometer (KM)', 'Doors']]
y = df['Price']

#Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

#Standardize the features (important for neural networks)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

#Build a simple neural network
model = tfk.Sequential([
    tfk.layers.Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)),
    tfk.layers.Dense(32, activation='relu'),
    tfk.layers.Dense(1)  #Output layer for regression task
])

#Compile the model
model.compile(optimizer='adam', loss='mean_squared_error')

#Train the model
model.fit(X_train_scaled, y_train, epochs=50, batch_size=32, validation_split=0.2)

#Evaluate the model on the test set
mse = model.evaluate(X_test_scaled, y_test)
print(f"Mean Squared Error on Test Set: {mse}")

#Make predictions
predictions = model.predict(X_test_scaled)
print(predictions)

# Application of k‐MEANS CLUSTERING

df = dataset.dropna()

#Select relevant features for clustering (you can customize this based on your needs)
features = df[['Odometer (KM)', 'Doors', 'Price']]

#Standardize the features
scaler = StandardScaler()
features_scaled = scaler.fit_transform(features)

#Choose the number of clusters (you can adjust this based on your problem)
num_clusters = 3

#Apply k-Means clustering
kmeans = KMeans(n_clusters=num_clusters, random_state=42)
df['Cluster'] = kmeans.fit_predict(features_scaled)

#Display the resulting clusters
print(df[['Odometer (KM)', 'Doors', 'Price', 'Cluster']])

#Visualize the clusters (for 2D data)
plt.scatter(features_scaled[:, 0], features_scaled[:, 1], c=df['Cluster'], cmap='viridis')
plt.title('k-Means Clustering')
plt.xlabel('Odometer (Scaled)')
plt.ylabel('Doors (Scaled)')
plt.show()

# Estimation Model Evaluation

df = dataset.dropna()

#Define features and target variable
X = df[['Odometer (KM)', 'Doors']]
y = df['Price']

#Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

#Initialize and train the linear regression model
model = LinearRegression()
model.fit(X_train, y_train)

#Make predictions on the test set
y_pred = model.predict(X_test)

#Evaluate the model using various metrics
mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

#Display the evaluation metrics
print(f"Mean Absolute Error: {mae:.2f}")
print(f"Mean Squared Error: {mse:.2f}")
print(f"R-squared: {r2:.2f}")

# Demonstrate How you will Identify Multicollinearity

df = dataset.dropna()

#Define features
X = df[['Odometer (KM)', 'Doors', 'Price']]

#Add a constant term for the intercept
X = sm.add_constant(X)

#Calculate VIF for each variable
vif_data = pd.DataFrame()
vif_data["Variable"] = X.columns
vif_data["VIF"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]

print(vif_data)

# Demonstrate HOW you’ll apply PRINCIPAL COMPONENTS ANALYSIS

df = dataset.dropna()

#Define features
features = df[['Odometer (KM)', 'Doors', 'Price']]

#Standardize the features
scaler = StandardScaler()
features_scaled = scaler.fit_transform(features)

#Apply PCA
pca = PCA()
principal_components = pca.fit_transform(features_scaled)

#Create a DataFrame to visualize the results
pc_df = pd.DataFrame(data=principal_components, columns=['PC1', 'PC2', 'PC3'])

#Display the principal components DataFrame
print(pc_df.head())

#Percentage of variance explained by each principal component
explained_variance_ratio = pca.explained_variance_ratio_
print("Explained Variance Ratios:", explained_variance_ratio)

#Cumulative variance explained
cumulative_variance_ratio = explained_variance_ratio.cumsum()
print("Cumulative Variance Explained:", cumulative_variance_ratio)

"""# New Section"""

from google.colab import files
uploaded = files.upload()

cdata = pd.read_csv("credit_record.csv")
creditData = cdata[:8000]
print(creditData.head)

# Partition the Data

X = creditData[['ID', 'MONTHS_BALANCE']]
y = creditData['STATUS']

#Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
print(X_train)

# Balance the Training Data Set

X = creditData[['ID', 'MONTHS_BALANCE']]
y = creditData['STATUS']

#Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

#Use Random Over-Sampling to balance the training dataset
oversampler = RandomOverSampler(random_state=42)
X_train_resampled, y_train_resampled = oversampler.fit_resample(X_train, y_train)
print(X_train_resampled)

# Build CART Decision Trees

X = creditData[['ID', 'MONTHS_BALANCE']]
y = creditData['STATUS']

#Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

#Initialize the Decision Tree Classifier
tree_classifier = DecisionTreeClassifier(random_state=42)

#Train the model
tree_classifier.fit(X_train, y_train)

#Make predictions on the test set
y_pred = tree_classifier.predict(X_test)

#Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)
classification_rep = classification_report(y_test, y_pred)


print("Accuracy:", accuracy)
print("Confusion Matrix:\n", conf_matrix)
print("Classification Report:\n", classification_rep)

plt.figure(figsize=(12, 8))
plot_tree(tree_classifier, filled=True, feature_names=X.columns, class_names=tree_classifier.classes_)
plt.show()

# Build Random Forests

X = creditData[['ID', 'MONTHS_BALANCE']]
y = creditData['STATUS']

#Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

#Initialize the Random Forest Classifier
rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)

#Train the model
rf_classifier.fit(X_train, y_train)

#Make predictions on the test set
y_pred = rf_classifier.predict(X_test)

#Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)
classification_rep = classification_report(y_test, y_pred)

print("Accuracy:", accuracy)
print("Confusion Matrix:\n", conf_matrix)
print("Classification Report:\n", classification_rep)

# Perform Logistic Regression

X = cdata[['MONTHS_BALANCE']]  #Features
y = cdata['STATUS']  #Target variable

#Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

#Initialize the logistic regression model
model = LogisticRegression()

#Train the model
model.fit(X_train, y_train)

#Make predictions on the test set
y_pred = model.predict(X_test)

#Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)
classification_rep = classification_report(y_test, y_pred)

print("Accuracy:", accuracy)
print("Confusion Matrix:\n", conf_matrix)
print("Classification Report:\n", classification_rep)

# Perform Poisson Regression

cdata['COUNT'] = 1  #Creating a count variable for each observation
X = cdata[['MONTHS_BALANCE']]  #Features
y = cdata['COUNT']  #Target variable

#Fit Poisson regression model
poisson_model = sm.GLM(y, X, family=sm.families.Poisson()).fit()

print(poisson_model.summary())

"""# New Section"""

from google.colab import files
uploaded = files.upload()

sdata = pd.read_csv("Bakery.csv")
shopData=sdata[:8000]
print(shopData)

#  Mine Association Rules

transactions = shopData.groupby('TransactionNo')['Items'].apply(list).values.tolist()

#Convert the transaction data into a one-hot encoded DataFrame
te = TransactionEncoder()
te_ary = te.fit(transactions).transform(transactions)
df_encoded = pd.DataFrame(te_ary, columns=te.columns_)

#Use Apriori algorithm to find frequent itemsets
frequent_itemsets = apriori(df_encoded, min_support=0.01, use_colnames=True)

#Generate association rules
rules = association_rules(frequent_itemsets, metric="confidence", min_threshold=0.5)

print(rules)

# Apply the Confidence Difference Criterion

transactions = shopData.groupby('TransactionNo')['Items'].apply(list).values.tolist()

#Convert the transaction data into a one-hot encoded DataFrame
te = TransactionEncoder()
te_ary = te.fit(transactions).transform(transactions)
df_encoded = pd.DataFrame(te_ary, columns=te.columns_)

#Use Apriori algorithm to find frequent itemsets
frequent_itemsets = apriori(df_encoded, min_support=0.01, use_colnames=True)

#Generate association rules
rules = association_rules(frequent_itemsets, metric="confidence", min_threshold=0.5)

#Apply Confidence Difference Criterion
min_confidence_diff = 0.1

#Calculate confidence differences
rules['confidence_diff'] = rules.apply(lambda row: row['confidence'] - row['antecedent support'], axis=1)

#Filter rules based on the confidence difference criterion
filtered_rules = rules[rules['confidence_diff'] >= min_confidence_diff]

print(filtered_rules)

# Apply the Confidence Quotient Criterion

transactions = shopData.groupby('TransactionNo')['Items'].apply(list).values.tolist()

#Convert the transaction data into a one-hot encoded DataFrame
te = TransactionEncoder()
te_ary = te.fit(transactions).transform(transactions)
df_encoded = pd.DataFrame(te_ary, columns=te.columns_)

#Use Apriori algorithm to find frequent itemsets
frequent_itemsets = apriori(df_encoded, min_support=0.01, use_colnames=True)

#Generate association rules
rules = association_rules(frequent_itemsets, metric="confidence", min_threshold=0.5)

#Apply Confidence Quotient Criterion
min_confidence_quotient = 1.5

#Calculate confidence quotient
rules['confidence_quotient'] = rules['confidence'] / rules['antecedent support']

#Filter rules based on the confidence quotient criterion
filtered_rules = rules[rules['confidence_quotient'] >= min_confidence_quotient]

print(filtered_rules)